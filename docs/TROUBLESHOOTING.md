# Troubleshooting Guide

## Common Issues and Solutions

### 1. "ImportError: No module named 'langgraph'"

**Solution:**
```bash
pip install --upgrade -r requirements.txt
```

Make sure your virtual environment is activated.

---

### 2. "OpenAlex search failed: 429 Too Many Requests"

**Cause:** Rate limiting

**Solutions:**
- Ensure `OPENALEX_EMAIL` is set (enables polite pool with 10x rate limit)
- Increase `rate_limit_delay` in `openalex_client.py`
- Reduce number of queries generated by Abstractor

---

### 3. "No papers found" or "All papers scored below threshold"

**Possible causes:**
- Search queries too narrow
- Date range too restrictive (only last 7 days by default)
- LLM generating poor queries

**Solutions:**
- Increase `SEARCH_DAYS_BACK` in `.env` (e.g., 14 or 30 days)
- Check generated queries in logs - are they reasonable?
- Refine the ABSTRACTOR prompt in `src/core/prompts.py`
- Lower the borrowability threshold in `src/agents/analyst.py` (currently 0.5)

---

### 4. "Failed to extract text from PDF"

**Cause:** Corrupted or image-only PDFs

**Solutions:**
- Ensure PDFs are text-based (not scanned images)
- Try opening the PDF manually to verify it's valid
- Use OCR preprocessing if needed
- Check if file is actually a PDF: `file your_paper.pdf`

---

### 5. "SMTP authentication failed"

**For Gmail users:**
- Use an "App Password", not your regular Gmail password
- Enable 2FA on your Google account
- Generate App Password at: https://myaccount.google.com/apppasswords

**For other providers:**
- Check SMTP host and port in `.env`
- Common ports: 587 (TLS), 465 (SSL), 25 (plain)

---

### 6. "LLM response could not be parsed"

**Cause:** LLM returned malformed JSON

**Solutions:**
- Check if you're hitting token limits (use shorter PDFs)
- Try a different model in `.env` (LLM_MODEL)
- Lower temperature for more consistent output
- Check logs for raw LLM response

**Temporary fix:**
```python
# In agent files, add better error handling:
try:
    data = json.loads(response_text)
except json.JSONDecodeError as e:
    logger.error(f"JSON parse error: {e}")
    logger.error(f"Raw response: {response_text}")
    # Implement retry or fallback
```

---

### 7. "GitHub Actions workflow fails"

**Check:**
1. All required secrets are set in repository settings
2. Profile is committed to the repository (`cache/profile.json`)
3. Check workflow logs for specific error messages

**Common fixes:**
- Re-run the workflow manually to test
- Verify secrets don't have extra spaces or newlines
- Make sure repository is public (or Actions enabled for private repo)

---

### 8. Profile generation produces poor results

**Symptoms:**
- Vague "core task" like "Machine learning research"
- Generic pain points
- Missing domain keywords

**Solutions:**
- Use more PDFs (3-5 recommended)
- Ensure PDFs are your actual research papers, not reviews
- Edit `PROFILER_SYSTEM_PROMPT` to be more specific to your field
- Manually edit `cache/profile.json` to improve it

---

### 9. Memory errors when processing PDFs

**Solutions:**
- Limit pages extracted per PDF (currently 10, can reduce)
- Process fewer PDFs at once (currently max 5)
- Reduce `max_chars` in `smart_truncate()`

---

### 10. Email not arriving

**Check:**
1. Spam/junk folder
2. SMTP logs in `academic_radar.log`
3. Recipient email is correct
4. SMTP credentials are valid

**Test manually:**
```python
python -c "
from src.agents.publisher import PublisherAgent
agent = PublisherAgent()
# Test will fail if SMTP is misconfigured
"
```

---

## Getting Help

1. **Check logs**: `academic_radar.log` has detailed information
2. **Enable debug logging**: In `main.py`, change `level=logging.INFO` to `level=logging.DEBUG`
3. **Test components individually**: Run each agent separately
4. **Open an issue**: Provide logs and configuration (redact API keys!)

## Performance Tips

### Speed up execution:
- Use faster LLM models (gpt-3.5-turbo instead of gpt-4)
- Reduce `MAX_PAPERS_TO_ANALYZE`
- Cache profile and reuse it
- Run search queries in parallel (requires code modification)

### Improve quality:
- Use better LLMs (gpt-4, Claude Opus)
- Increase `MAX_PAPERS_TO_ANALYZE` to 10-20
- Refine system prompts
- Download full PDFs for methodology extraction (requires modification)

### Reduce costs:
- Use Deepseek (much cheaper than OpenAI)
- Cache LLM responses
- Use smaller context windows
- Process fewer papers
